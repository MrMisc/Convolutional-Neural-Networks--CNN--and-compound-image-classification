{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project lll.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnv7R5yINioF"
      },
      "source": [
        "## 1D convolution (implementation should  not directly use model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yvcxASlOOd5"
      },
      "source": [
        "1. Use linear discrete 1D convolution to create a database with at least 1000 entries.  Each entry contains one of five \"sounds\". For each entry, linearly convolve a new instance of 2000 identical, independently distributed Gaussian samples with one of 5 different filters(impulse response, kernels). If you play the files over a loudspeaker (8kHz sampling) they should sound different.  [One method: Take a Hamming window shape (readily available) of suitable length (short is recommended) as a first kernel (filter). Then multiply said kernel by sin(wt) where t is time, with different frequencies w between 0 and pi, to create 4 more filters, each passing through a different frequency band. \n",
        "2. Design a simple 1 dimensional convolutional network using convolution, pooling and other tools with a one-hot output that classifies the sounds in the file. \n",
        "3. Test and optimise your setup with suitable methods. \n",
        "4. Extend this toy study in one direction of your choice to provide additional insight in the behaviour of 1 dimensional convolution (for eg, what happens with mixed signals, different file lengths, distorted filters, batch normalization etc). Provide a solid motivation and reasoning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF0TH9swPkiD"
      },
      "source": [
        "#### OR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL7WKjUfPl8J"
      },
      "source": [
        "### Basic 2D convolution (implementation should  not directly use model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMTs5k0DPqmb"
      },
      "source": [
        "1. Create a database of 28*28 images with at least 1000 images. The database images each contain one circle, one triangle, or one rectangle. The triangles and rectangles can be all of the same oritentation, size, and shape but should have random locations. \n",
        "2. Design a simple convolutional network using convolution, pooling and other tools with a one-hot output to detect the shape of the objects. \n",
        "3. Test and optimize your setup with suitable methods. \n",
        "4. Extend this toy study in  one direction of your choice to provide additional insight in the behaviour of 2 dimensional convolution  (for eg, what happens with object distortions, varying object orientation, multiple objects, batch normalisation , etc). Provide solid motivation and reasoning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bCMw190N4oe"
      },
      "source": [
        "Replacing model.fit has typically been done as follows below "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cp-moXYNgzJ"
      },
      "source": [
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)) #x and y are training datasets to be defined beforehand\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((xt, yt)) #xt and yt are test datasets to be defined beforehand\n",
        "\n",
        "train_dataset = train_dataset.batch(50)\n",
        "test_dataset = test_dataset.batch(25)\n",
        "\n",
        "# optimizer and loss function to train\n",
        "loss_object = tf.keras.losses.KLDivergence()  #Can experiment with this loss object accordingly to try to get better fits\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Defining our metrics\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "train_accuracy = tf.keras.metrics.KLDivergence('train_accuracy')   #If loss object is to be experimented with, would be ideal to match this with the above loss_object\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "test_accuracy = tf.keras.metrics.KLDivergence('test_accuracy')    #If loss object is to be experimented with, would be ideal to match this with the above loss_object\n",
        "\n",
        "def train_step(model, optimizer, x_train, y_train):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(x_train, training=True)\n",
        "    loss = loss_object(y_train, predictions)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(y_train, predictions)\n",
        "\n",
        "def test_step(model, x_test, y_test):\n",
        "  predictions = model(x_test)\n",
        "  loss = loss_object(y_test, predictions)\n",
        "\n",
        "  test_loss(loss)\n",
        "  test_accuracy(y_test, predictions)\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "\n",
        "def my_modelf1():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(2),\n",
        "    tf.keras.layers.Dense(200, kernel_regularizer=tf.keras.regularizers.l2(1e-5),activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "     tf.keras.layers.Dense(200, kernel_regularizer=tf.keras.regularizers.l2(1e-5),activation='relu'),\n",
        "    tf.keras.layers.Dense(2,name=\"predictions\", activation='linear')\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVOi6IeVOB1p"
      },
      "source": [
        "#running training on model as well as evaluating how well it fits on test data\n",
        "modelf1 = my_modelf1()\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for (x, y) in train_dataset:\n",
        "    train_step(modelf1, optimizer, x, y)\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "\n",
        "  for (xt, yt) in test_dataset:\n",
        "    test_step(modelf1, xt, yt)\n",
        "  with test_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "  \n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print (template.format(epoch+1,\n",
        "                         train_loss.result(), \n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(), \n",
        "                         test_accuracy.result()*100))\n",
        "\n",
        "  # Reset metrics every epoch\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}